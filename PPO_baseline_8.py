import pandas as pd
import os
import torch
import logging
import torch.nn as nn
import numpy as np
from convlab2.util.train_util import to_device
import torch.nn as nn
from torch import optim
import zipfile
import sys
import matplotlib.pyplot  as plt
import pickle
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import torch.tensor as tensor
import copy

# baseline = \
# [[0.555, 0.6, 0.585, 0.625, 0.625, 0.655, 0.665, 0.675, 0.695, 0.7, 0.695, 0.695, 0.685, 0.695, 0.695, 0.69, 0.685, 0.705, 0.7, 0.725, 0.715, 0.71, 0.695, 0.71, 0.7, 0.725, 0.715, 0.73, 0.705, 0.735],
# [0.565, 0.57, 0.59, 0.59, 0.635, 0.67, 0.665, 0.675, 0.68, 0.68, 0.685, 0.695, 0.7, 0.685, 0.685, 0.69, 0.69, 0.685, 0.695, 0.685, 0.695, 0.69, 0.695, 0.7, 0.695, 0.7, 0.7, 0.72, 0.72, 0.715],
# [0.555, 0.56, 0.56, 0.565, 0.57, 0.57, 0.61, 0.625, 0.64, 0.635, 0.685, 0.685, 0.68, 0.71, 0.7, 0.71, 0.72, 0.705, 0.725, 0.735, 0.74, 0.745, 0.73, 0.72, 0.71, 0.72, 0.72, 0.71, 0.705, 0.72],
# [0.56, 0.57, 0.58, 0.605, 0.635, 0.65, 0.665, 0.665, 0.66, 0.68, 0.675, 0.67, 0.68, 0.68, 0.68, 0.715, 0.725, 0.675, 0.7, 0.685, 0.67, 0.67, 0.695, 0.7, 0.71, 0.71, 0.705, 0.705, 0.705, 0.7],
# [0.57, 0.58, 0.59, 0.625, 0.65, 0.655, 0.66, 0.675, 0.665, 0.665, 0.665, 0.675, 0.705, 0.705,0.71, 0.715, 0.74, 0.73, 0.74, 0.735, 0.73, 0.735, 0.745, 0.735, 0.73, 0.72, 0.73, 0.72, 0.71, 0.68],
# [0.57, 0.57, 0.585, 0.6, 0.59, 0.605, 0.62, 0.62, 0.63, 0.645, 0.67, 0.635, 0.63, 0.65, 0.665, 0.67, 0.66, 0.67, 0.655, 0.655, 0.69, 0.69, 0.675, 0.665, 0.64, 0.64, 0.635, 0.65, 0.66, 0.655],
# [0.57, 0.565, 0.58, 0.59, 0.61, 0.605, 0.655, 0.645, 0.645, 0.645, 0.675, 0.68, 0.69, 0.69, 0.69, 0.69, 0.69, 0.685, 0.68, 0.675, 0.645, 0.645, 0.63, 0.635, 0.665, 0.67, 0.665, 0.65, 0.66, 0.66],
# [0.565, 0.58, 0.61, 0.62, 0.655, 0.645, 0.64, 0.645, 0.645, 0.665, 0.665, 0.7, 0.665, 0.695, 0.67, 0.685, 0.695, 0.72, 0.7, 0.685, 0.705, 0.7, 0.7, 0.71, 0.715, 0.705, 0.72, 0.715, 0.7, 0.68]]

# baseline1 = [[0.595, 0.64, 0.625, 0.685, 0.685, 0.67, 0.705, 0.73, 0.725, 0.73, 0.74, 0.76, 0.745, 0.72, 0.715, 0.735, 0.74, 0.735, 0.71, 0.72, 0.715, 0.72, 0.715, 0.695, 0.7, 0.735, 0.715, 0.71, 0.73, 0.72]
# ,[0.615, 0.63, 0.665, 0.685, 0.69, 0.705, 0.695, 0.68, 0.705, 0.675, 0.665, 0.69, 0.675, 0.68, 0.685, 0.69, 0.67, 0.645, 0.655, 0.64, 0.64, 0.645, 0.655, 0.575, 0.525, 0.505, 0.525, 0.565, 0.57, 0.555]
# ,[0.61, 0.63, 0.65, 0.67, 0.665, 0.685, 0.685, 0.705, 0.73, 0.74, 0.72, 0.725, 0.715, 0.69, 0.68, 0.68, 0.68, 0.685, 0.67, 0.655, 0.66, 0.675, 0.67, 0.67, 0.685, 0.695, 0.675, 0.67, 0.63, 0.635]
# ,[0.625, 0.665, 0.67, 0.655, 0.655, 0.71, 0.7, 0.675, 0.65, 0.7, 0.7, 0.705, 0.655, 0.685, 0.68, 0.72, 0.71, 0.7, 0.715, 0.725, 0.715, 0.695, 0.705, 0.7, 0.71, 0.685, 0.67, 0.685, 0.675, 0.675]
# ,[0.56, 0.6, 0.625, 0.635, 0.63, 0.65, 0.64, 0.63, 0.625, 0.665, 0.64, 0.635, 0.635, 0.58, 0.59, 0.615, 0.605, 0.535, 0.555, 0.58, 0.56, 0.515, 0.525, 0.505, 0.495, 0.505, 0.525, 0.525, 0.505, 0.51]
# ,[0.56, 0.58, 0.575, 0.595, 0.62, 0.62, 0.65, 0.645, 0.655, 0.645, 0.67, 0.645, 0.685, 0.655, 0.69, 0.695, 0.71, 0.695, 0.68, 0.675, 0.68, 0.685, 0.65, 0.615, 0.635, 0.675, 0.67, 0.67, 0.695, 0.67]
# ,[0.58, 0.595, 0.59, 0.6, 0.64, 0.61, 0.625, 0.635, 0.62, 0.64, 0.655, 0.645, 0.65, 0.66, 0.635, 0.585, 0.59, 0.62, 0.625, 0.6, 0.63, 0.58, 0.625, 0.61, 0.6, 0.645, 0.6, 0.6, 0.595, 0.595]
# ,[0.605, 0.61, 0.65, 0.645, 0.665, 0.685, 0.7, 0.71, 0.695, 0.72, 0.705, 0.7, 0.72, 0.725, 0.69, 0.73, 0.735, 0.695, 0.71, 0.69, 0.675, 0.68, 0.7, 0.68, 0.68, 0.68, 0.68, 0.66, 0.655, 0.67]]

# baseline2 = [[0.595, 0.635, 0.625, 0.64, 0.62, 0.7, 0.705, 0.73, 0.705, 0.685, 0.675, 0.68, 0.675, 0.65, 0.625, 0.605, 0.565, 0.585, 0.57, 0.565, 0.585, 0.605, 0.605, 0.61, 0.58, 0.58, 0.56, 0.555, 0.58, 0.56]
# ,[0.61, 0.605, 0.645, 0.655, 0.665, 0.675, 0.685, 0.71, 0.72, 0.71, 0.72, 0.725, 0.74, 0.735, 0.73, 0.73, 0.72, 0.7, 0.68, 0.64, 0.68, 0.7, 0.69, 0.65, 0.685, 0.675, 0.645, 0.655, 0.675, 0.665]
# ,[0.58, 0.56, 0.565, 0.595, 0.595, 0.62, 0.625, 0.61, 0.66, 0.64, 0.64, 0.61, 0.61, 0.665, 0.65, 0.675, 0.65, 0.67, 0.62, 0.64, 0.65, 0.625, 0.65, 0.635, 0.655, 0.66, 0.65, 0.66, 0.66, 0.675]
# ,[0.6, 0.615, 0.625, 0.66, 0.675, 0.69, 0.7, 0.695, 0.7, 0.725, 0.715, 0.73, 0.74, 0.725, 0.725, 0.74, 0.72, 0.695, 0.715, 0.68, 0.685, 0.675, 0.665, 0.665, 0.68, 0.665, 0.68, 0.615, 0.665, 0.66]
# ,[0.575, 0.595, 0.62, 0.65, 0.675, 0.635, 0.65, 0.655, 0.685, 0.67, 0.69, 0.66, 0.675, 0.66, 0.71, 0.68, 0.7, 0.69, 0.715, 0.72, 0.73, 0.72, 0.705, 0.695, 0.725, 0.715, 0.705, 0.685, 0.705, 0.66]
# ,[0.58, 0.605, 0.635, 0.65, 0.66, 0.68, 0.665, 0.645, 0.635, 0.66, 0.65, 0.685, 0.67, 0.66, 0.67, 0.63, 0.645, 0.66, 0.695, 0.65, 0.605, 0.64, 0.68, 0.65, 0.635, 0.65, 0.685, 0.73, 0.71, 0.695]
# ,[0.615, 0.63, 0.65, 0.66, 0.655, 0.65, 0.66, 0.665, 0.655, 0.66, 0.71, 0.695, 0.695, 0.665, 0.65, 0.635, 0.635, 0.655, 0.63, 0.635, 0.625, 0.675, 0.645, 0.675, 0.66, 0.64, 0.63, 0.64, 0.65, 0.66]
# ,[0.615, 0.625, 0.62, 0.605, 0.58, 0.615, 0.615, 0.63, 0.625, 0.64, 0.665, 0.69, 0.68, 0.71, 0.685, 0.69, 0.695, 0.685, 0.72, 0.705, 0.725, 0.715, 0.695, 0.715, 0.73, 0.725, 0.72, 0.745, 0.725, 0.74]]

# baseline_global = [[0.62, 0.63, 0.645, 0.635, 0.665, 0.71, 0.7, 0.73, 0.715, 0.72, 0.69, 0.73, 0.695, 0.72, 0.7,0.73, 0.71, 0.705, 0.73, 0.72, 0.73, 0.71, 0.71, 0.72, 0.735, 0.7, 0.71, 0.72, 0.72, 0.73]
# ,[0.605, 0.63, 0.655, 0.685, 0.645, 0.635, 0.67, 0.705, 0.69, 0.715, 0.7, 0.715, 0.72, 0.72, 0.725, 0.73, 0.715, 0.725, 0.71, 0.71, 0.72, 0.67, 0.705, 0.715, 0.685, 0.69, 0.665, 0.665, 0.675, 0.68]
# ,[0.59, 0.615, 0.605, 0.68, 0.66, 0.7, 0.715, 0.695, 0.7, 0.695, 0.71, 0.705, 0.715, 0.695, 0.7, 0.68, 0.705, 0.69, 0.695, 0.67, 0.675, 0.675, 0.68, 0.68, 0.69, 0.69, 0.68, 0.685, 0.71, 0.695]
# ,[0.595, 0.605, 0.62, 0.63, 0.66, 0.69, 0.705, 0.7, 0.72, 0.695, 0.675, 0.71, 0.725, 0.715, 0.71, 0.7, 0.715, 0.725, 0.705, 0.71, 0.685, 0.69, 0.69, 0.69, 0.685, 0.68, 0.695, 0.675, 0.66, 0.675]
# ,[0.595, 0.605, 0.61, 0.63, 0.655, 0.69, 0.69, 0.68, 0.695, 0.67, 0.65, 0.655, 0.68, 0.645, 0.7, 0.695, 0.695, 0.7, 0.67, 0.67, 0.665, 0.67, 0.67, 0.655, 0.66, 0.65, 0.675, 0.68, 0.675, 0.675]
# ,[0.58, 0.6, 0.64, 0.655, 0.655, 0.67, 0.67, 0.685, 0.71, 0.695, 0.705, 0.72, 0.74, 0.75, 0.71, 0.715, 0.72, 0.705, 0.69, 0.72, 0.715, 0.705, 0.685, 0.68, 0.69, 0.675, 0.665, 0.69, 0.67, 0.69]
# ,[0.61, 0.595, 0.665, 0.65, 0.665, 0.645, 0.655, 0.67, 0.68, 0.68, 0.675, 0.68, 0.675, 0.66, 0.665, 0.665, 0.68, 0.67, 0.67, 0.64, 0.645, 0.66, 0.655, 0.665, 0.695, 0.69, 0.69, 0.695, 0.68, 0.685]]
global_reward = [[0.625, 0.655, 0.67, 0.71, 0.705, 0.715, 0.715, 0.72, 0.71, 0.685, 0.685, 0.695, 0.655, 0.68, 0.645, 0.625, 0.63, 0.67, 0.67, 0.67, 0.685, 0.66, 0.66, 0.65, 0.65, 0.665, 0.655, 0.66, 0.67, 0.635]
,[0.605, 0.635, 0.635, 0.62, 0.635, 0.66, 0.655, 0.64, 0.66, 0.615, 0.67, 0.635, 0.675, 0.635, 0.635, 0.63, 0.665, 0.66, 0.645, 0.655, 0.685, 0.665, 0.645, 0.61, 0.615, 0.625, 0.645, 0.64, 0.655, 0.65]
,[0.605, 0.58, 0.625, 0.65, 0.62, 0.605, 0.615, 0.64, 0.635, 0.67, 0.65, 0.665, 0.67, 0.68, 0.68, 0.72, 0.725, 0.685, 0.685, 0.695, 0.665, 0.655, 0.655, 0.645, 0.655, 0.655, 0.655, 0.65, 0.645, 0.635]
,[0.59, 0.645, 0.635, 0.665, 0.68, 0.705, 0.68, 0.675, 0.67, 0.66, 0.69, 0.715, 0.695, 0.74,0.73, 0.735, 0.74, 0.705, 0.71, 0.715, 0.71, 0.69, 0.69, 0.675, 0.705, 0.695, 0.675, 0.7, 0.695, 0.7]]
baseline_np = np.array(global_reward)
mean = np.mean(baseline_np,axis = 0)
data_df = pd.DataFrame(mean)

# begin writing to xls
writer = pd.ExcelWriter('Save_Excel.xlsx')
data_df.to_excel(writer,'page_1',float_format='%.5f') # float_format 控制精度
writer.save()
print(mean)
# begin drawing
axis = [i for i in range(len(mean))]
plt.plot(axis, mean)
my_y_ticks = np.arange(0.5, 1, 0.05)
plt.yticks(my_y_ticks)
plt.xlabel('Number of Epoch')
plt.ylabel('Success rate')
plt.show()