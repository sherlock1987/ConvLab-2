import pandas as pd
import os
import torch
import logging
import torch.nn as nn
import numpy as np
from convlab2.util.train_util import to_device
import torch.nn as nn
from torch import optim
import zipfile
import sys
import matplotlib.pyplot  as plt
import pickle
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import torch.tensor as tensor
import copy


# idea4 pretrained G  *10

# idea4 pretrained G
# [0.608125, 0.63625, 0.67375, 0.705625, 0.72125, 0.725625, 0.7424999999999999, 0.7387500000000001, 0.741875, 0.75125, 0.7493750000000001, 0.7512500000000001, 0.75125, 0.7531249999999999, 0.750625, 0.7575, 0.753125, 0.7475, 0.75125, 0.7462500000000001, 0.7418750000000002, 0.73875, 0.7374999999999999, 0.7324999999999999, 0.72875, 0.72375, 0.7287499999999999, 0.731875, 0.7331249999999999, 0.7324999999999999, 0.7337499999999999, 0.7443750000000001, 0.735625, 0.7331249999999999, 0.7337499999999998, 0.72625, 0.730625, 0.7306250000000001, 0.7293749999999999, 0.73125]
# idea4 D_49

# idea4 pretrained D reward = log(D)

# idea4 D_49         reward = log(D)

# idea4 D_25         reward = log(D)
# PPO_no_MLE + no reward
#  [0.31500000000000006, 0.31875, 0.31375000000000003, 0.308125, 0.29875, 0.30000000000000004, 0.30500000000000005, 0.30125, 0.306875, 0.318125, 0.30562500000000004, 0.31249999999999994, 0.2975, 0.30062500000000003, 0.295, 0.30250000000000005, 0.29500000000000004, 0.29874999999999996, 0.30374999999999996, 0.2975, 0.3025, 0.29125, 0.28625, 0.2875, 0.28812499999999996, 0.28437500000000004, 0.284375, 0.29124999999999995, 0.28875, 0.286875, 0.289375, 0.303125, 0.30000000000000004, 0.3025, 0.29937499999999995, 0.301875, 0.313125, 0.30874999999999997, 0.31125, 0.30437500000000006, 0.29937499999999995, 0.295625, 0.298125, 0.30187499999999995, 0.30562500000000004, 0.30125, 0.29625, 0.29125, 0.3, 0.301875, 0.3025, 0.301875, 0.305625, 0.31499999999999995, 0.31250000000000006, 0.31125, 0.311875, 0.306875, 0.314375, 0.30875]

# PPO_no_MLE + reward(D)
# [0.33125000000000004, 0.331875, 0.33437500000000003, 0.32875000000000004, 0.328125, 0.34187500000000004, 0.339375, 0.344375, 0.34625, 0.344375, 0.3375, 0.32625, 0.31999999999999995, 0.3225, 0.319375, 0.32375, 0.32437499999999997, 0.315, 0.32, 0.31375000000000003, 0.326875, 0.320625, 0.32562500000000005, 0.323125, 0.3325, 0.33875, 0.338125, 0.3425, 0.331875, 0.333125, 0.32875, 0.323125, 0.334375, 0.33000000000000007, 0.32937500000000003, 0.334375, 0.3275, 0.33625, 0.335, 0.328125, 0.33249999999999996, 0.33312499999999995, 0.336875, 0.33875, 0.33312500000000006, 0.33187500000000003, 0.33625, 0.33249999999999996, 0.34625, 0.3375, 0.33875, 0.33999999999999997, 0.344375, 0.345, 0.338125, 0.338125, 0.338125, 0.34562499999999996, 0.344375, 0.33562499999999995]
# PPO
# [0.6087499999999999, 0.658125, 0.6825000000000001, 0.7106250000000001, 0.7262499999999998, 0.739375, 0.7468750000000001, 0.74875, 0.75, 0.7512500000000001, 0.7537500000000001, 0.745, 0.754375, 0.74875, 0.750625, 0.749375, 0.749375, 0.74875, 0.7525, 0.74875, 0.7450000000000001, 0.744375, 0.7318749999999999, 0.7393750000000001, 0.73125, 0.73, 0.7350000000000001, 0.734375, 0.7406250000000001, 0.72875, 0.7293749999999999, 0.73875, 0.736875, 0.738125, 0.74, 0.7418750000000001, 0.73625, 0.74, 0.739375, 0.74125]
baseline=\
    [0.6087499999999999, 0.658125, 0.6825000000000001, 0.7106250000000001, 0.7262499999999998, 0.739375, 0.7468750000000001, 0.74875, 0.75, 0.7512500000000001, 0.7537500000000001, 0.745, 0.754375, 0.74875, 0.750625, 0.749375, 0.749375, 0.74875, 0.7525, 0.74875, 0.7450000000000001, 0.744375, 0.7318749999999999, 0.7393750000000001, 0.73125, 0.73, 0.7350000000000001, 0.734375, 0.7406250000000001, 0.72875, 0.7293749999999999, 0.73875, 0.736875, 0.738125, 0.74, 0.7418750000000001, 0.73625, 0.74, 0.739375, 0.74125]

idea4 = \
    [0.608125, 0.63625, 0.67375, 0.705625, 0.72125, 0.725625, 0.7424999999999999, 0.7387500000000001, 0.741875, 0.75125, 0.7493750000000001, 0.7512500000000001, 0.75125, 0.7531249999999999, 0.750625, 0.7575, 0.753125, 0.7475, 0.75125, 0.7462500000000001, 0.7418750000000002, 0.73875, 0.7374999999999999, 0.7324999999999999, 0.72875, 0.72375, 0.7287499999999999, 0.731875, 0.7331249999999999, 0.7324999999999999, 0.7337499999999999, 0.7443750000000001, 0.735625, 0.7331249999999999, 0.7337499999999998, 0.72625, 0.730625, 0.7306250000000001, 0.7293749999999999, 0.73125]

# # begin writing to xls
# writer = pd.ExcelWriter('Save_Excel.xlsx')
# data_df.to_excel(writer,'page_1',float_format='%.5f') # float_format 控制精度
# writer.save()
# print(list(mean))
# begin drawing
axis = [i for i in range(len(baseline))]
plt.plot(axis, baseline)
plt.plot(axis, idea4)
plt.legend(["baseline", "D_pre"], loc='upper right')
my_y_ticks = np.arange(0.6, 1, 0.05)
plt.yticks(my_y_ticks)
plt.xlabel('Number of Epoch')
plt.ylabel('Success rate')
plt.show()

