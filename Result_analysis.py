import pandas as pd
import os
import torch
import logging
import torch.nn as nn
import numpy as np
from convlab2.util.train_util import to_device
import torch.nn as nn
from torch import optim
import zipfile
import sys
import matplotlib.pyplot  as plt
import pickle
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import torch.tensor as tensor
import copy


# idea4 pretrained G  *10

# idea4 pretrained G
# [0.608125, 0.63625, 0.67375, 0.705625, 0.72125, 0.725625, 0.7424999999999999, 0.7387500000000001, 0.741875, 0.75125, 0.7493750000000001, 0.7512500000000001, 0.75125, 0.7531249999999999, 0.750625, 0.7575, 0.753125, 0.7475, 0.75125, 0.7462500000000001, 0.7418750000000002, 0.73875, 0.7374999999999999, 0.7324999999999999, 0.72875, 0.72375, 0.7287499999999999, 0.731875, 0.7331249999999999, 0.7324999999999999, 0.7337499999999999, 0.7443750000000001, 0.735625, 0.7331249999999999, 0.7337499999999998, 0.72625, 0.730625, 0.7306250000000001, 0.7293749999999999, 0.73125]
# idea4 D_49

# idea4 pretrained D reward = log(D)

# idea4 D_49         reward = log(D)

# idea4 D_25         reward = log(D)

# PPO
# [0.6087499999999999, 0.658125, 0.6825000000000001, 0.7106250000000001, 0.7262499999999998, 0.739375, 0.7468750000000001, 0.74875, 0.75, 0.7512500000000001, 0.7537500000000001, 0.745, 0.754375, 0.74875, 0.750625, 0.749375, 0.749375, 0.74875, 0.7525, 0.74875, 0.7450000000000001, 0.744375, 0.7318749999999999, 0.7393750000000001, 0.73125, 0.73, 0.7350000000000001, 0.734375, 0.7406250000000001, 0.72875, 0.7293749999999999, 0.73875, 0.736875, 0.738125, 0.74, 0.7418750000000001, 0.73625, 0.74, 0.739375, 0.74125]
baseline=\
    [0.6087499999999999, 0.658125, 0.6825000000000001, 0.7106250000000001, 0.7262499999999998, 0.739375, 0.7468750000000001, 0.74875, 0.75, 0.7512500000000001, 0.7537500000000001, 0.745, 0.754375, 0.74875, 0.750625, 0.749375, 0.749375, 0.74875, 0.7525, 0.74875, 0.7450000000000001, 0.744375, 0.7318749999999999, 0.7393750000000001, 0.73125, 0.73, 0.7350000000000001, 0.734375, 0.7406250000000001, 0.72875, 0.7293749999999999, 0.73875, 0.736875, 0.738125, 0.74, 0.7418750000000001, 0.73625, 0.74, 0.739375, 0.74125]

idea4 = \
    [0.608125, 0.63625, 0.67375, 0.705625, 0.72125, 0.725625, 0.7424999999999999, 0.7387500000000001, 0.741875, 0.75125, 0.7493750000000001, 0.7512500000000001, 0.75125, 0.7531249999999999, 0.750625, 0.7575, 0.753125, 0.7475, 0.75125, 0.7462500000000001, 0.7418750000000002, 0.73875, 0.7374999999999999, 0.7324999999999999, 0.72875, 0.72375, 0.7287499999999999, 0.731875, 0.7331249999999999, 0.7324999999999999, 0.7337499999999999, 0.7443750000000001, 0.735625, 0.7331249999999999, 0.7337499999999998, 0.72625, 0.730625, 0.7306250000000001, 0.7293749999999999, 0.73125]

# # begin writing to xls
# writer = pd.ExcelWriter('Save_Excel.xlsx')
# data_df.to_excel(writer,'page_1',float_format='%.5f') # float_format 控制精度
# writer.save()
# print(list(mean))
# begin drawing
axis = [i for i in range(len(baseline))]
plt.plot(axis, baseline)
plt.plot(axis, idea4)
plt.legend(["baseline", "D_pre"], loc='upper right')
my_y_ticks = np.arange(0.6, 1, 0.05)
plt.yticks(my_y_ticks)
plt.xlabel('Number of Epoch')
plt.ylabel('Success rate')
plt.show()

