import pandas as pd
import os
import torch
import logging
import torch.nn as nn
import numpy as np
from convlab2.util.train_util import to_device
import torch.nn as nn
from torch import optim
import zipfile
import sys
import matplotlib.pyplot  as plt
import pickle
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
import torch.tensor as tensor
import copy

# baseline = \
# [[0.555, 0.6, 0.585, 0.625, 0.625, 0.655, 0.665, 0.675, 0.695, 0.7, 0.695, 0.695, 0.685, 0.695, 0.695, 0.69, 0.685, 0.705, 0.7, 0.725, 0.715, 0.71, 0.695, 0.71, 0.7, 0.725, 0.715, 0.73, 0.705, 0.735],
# [0.565, 0.57, 0.59, 0.59, 0.635, 0.67, 0.665, 0.675, 0.68, 0.68, 0.685, 0.695, 0.7, 0.685, 0.685, 0.69, 0.69, 0.685, 0.695, 0.685, 0.695, 0.69, 0.695, 0.7, 0.695, 0.7, 0.7, 0.72, 0.72, 0.715],
# [0.555, 0.56, 0.56, 0.565, 0.57, 0.57, 0.61, 0.625, 0.64, 0.635, 0.685, 0.685, 0.68, 0.71, 0.7, 0.71, 0.72, 0.705, 0.725, 0.735, 0.74, 0.745, 0.73, 0.72, 0.71, 0.72, 0.72, 0.71, 0.705, 0.72],
# [0.56, 0.57, 0.58, 0.605, 0.635, 0.65, 0.665, 0.665, 0.66, 0.68, 0.675, 0.67, 0.68, 0.68, 0.68, 0.715, 0.725, 0.675, 0.7, 0.685, 0.67, 0.67, 0.695, 0.7, 0.71, 0.71, 0.705, 0.705, 0.705, 0.7],
# [0.57, 0.58, 0.59, 0.625, 0.65, 0.655, 0.66, 0.675, 0.665, 0.665, 0.665, 0.675, 0.705, 0.705,0.71, 0.715, 0.74, 0.73, 0.74, 0.735, 0.73, 0.735, 0.745, 0.735, 0.73, 0.72, 0.73, 0.72, 0.71, 0.68],
# [0.57, 0.57, 0.585, 0.6, 0.59, 0.605, 0.62, 0.62, 0.63, 0.645, 0.67, 0.635, 0.63, 0.65, 0.665, 0.67, 0.66, 0.67, 0.655, 0.655, 0.69, 0.69, 0.675, 0.665, 0.64, 0.64, 0.635, 0.65, 0.66, 0.655],
# [0.57, 0.565, 0.58, 0.59, 0.61, 0.605, 0.655, 0.645, 0.645, 0.645, 0.675, 0.68, 0.69, 0.69, 0.69, 0.69, 0.69, 0.685, 0.68, 0.675, 0.645, 0.645, 0.63, 0.635, 0.665, 0.67, 0.665, 0.65, 0.66, 0.66],
# [0.565, 0.58, 0.61, 0.62, 0.655, 0.645, 0.64, 0.645, 0.645, 0.665, 0.665, 0.7, 0.665, 0.695, 0.67, 0.685, 0.695, 0.72, 0.7, 0.685, 0.705, 0.7, 0.7, 0.71, 0.715, 0.705, 0.72, 0.715, 0.7, 0.68]]

# baseline1 = [[0.595, 0.64, 0.625, 0.685, 0.685, 0.67, 0.705, 0.73, 0.725, 0.73, 0.74, 0.76, 0.745, 0.72, 0.715, 0.735, 0.74, 0.735, 0.71, 0.72, 0.715, 0.72, 0.715, 0.695, 0.7, 0.735, 0.715, 0.71, 0.73, 0.72]
# ,[0.615, 0.63, 0.665, 0.685, 0.69, 0.705, 0.695, 0.68, 0.705, 0.675, 0.665, 0.69, 0.675, 0.68, 0.685, 0.69, 0.67, 0.645, 0.655, 0.64, 0.64, 0.645, 0.655, 0.575, 0.525, 0.505, 0.525, 0.565, 0.57, 0.555]
# ,[0.61, 0.63, 0.65, 0.67, 0.665, 0.685, 0.685, 0.705, 0.73, 0.74, 0.72, 0.725, 0.715, 0.69, 0.68, 0.68, 0.68, 0.685, 0.67, 0.655, 0.66, 0.675, 0.67, 0.67, 0.685, 0.695, 0.675, 0.67, 0.63, 0.635]
# ,[0.625, 0.665, 0.67, 0.655, 0.655, 0.71, 0.7, 0.675, 0.65, 0.7, 0.7, 0.705, 0.655, 0.685, 0.68, 0.72, 0.71, 0.7, 0.715, 0.725, 0.715, 0.695, 0.705, 0.7, 0.71, 0.685, 0.67, 0.685, 0.675, 0.675]
# ,[0.56, 0.6, 0.625, 0.635, 0.63, 0.65, 0.64, 0.63, 0.625, 0.665, 0.64, 0.635, 0.635, 0.58, 0.59, 0.615, 0.605, 0.535, 0.555, 0.58, 0.56, 0.515, 0.525, 0.505, 0.495, 0.505, 0.525, 0.525, 0.505, 0.51]
# ,[0.56, 0.58, 0.575, 0.595, 0.62, 0.62, 0.65, 0.645, 0.655, 0.645, 0.67, 0.645, 0.685, 0.655, 0.69, 0.695, 0.71, 0.695, 0.68, 0.675, 0.68, 0.685, 0.65, 0.615, 0.635, 0.675, 0.67, 0.67, 0.695, 0.67]
# ,[0.58, 0.595, 0.59, 0.6, 0.64, 0.61, 0.625, 0.635, 0.62, 0.64, 0.655, 0.645, 0.65, 0.66, 0.635, 0.585, 0.59, 0.62, 0.625, 0.6, 0.63, 0.58, 0.625, 0.61, 0.6, 0.645, 0.6, 0.6, 0.595, 0.595]
# ,[0.605, 0.61, 0.65, 0.645, 0.665, 0.685, 0.7, 0.71, 0.695, 0.72, 0.705, 0.7, 0.72, 0.725, 0.69, 0.73, 0.735, 0.695, 0.71, 0.69, 0.675, 0.68, 0.7, 0.68, 0.68, 0.68, 0.68, 0.66, 0.655, 0.67]]

# baseline2 = [[0.595, 0.635, 0.625, 0.64, 0.62, 0.7, 0.705, 0.73, 0.705, 0.685, 0.675, 0.68, 0.675, 0.65, 0.625, 0.605, 0.565, 0.585, 0.57, 0.565, 0.585, 0.605, 0.605, 0.61, 0.58, 0.58, 0.56, 0.555, 0.58, 0.56]
# ,[0.61, 0.605, 0.645, 0.655, 0.665, 0.675, 0.685, 0.71, 0.72, 0.71, 0.72, 0.725, 0.74, 0.735, 0.73, 0.73, 0.72, 0.7, 0.68, 0.64, 0.68, 0.7, 0.69, 0.65, 0.685, 0.675, 0.645, 0.655, 0.675, 0.665]
# ,[0.58, 0.56, 0.565, 0.595, 0.595, 0.62, 0.625, 0.61, 0.66, 0.64, 0.64, 0.61, 0.61, 0.665, 0.65, 0.675, 0.65, 0.67, 0.62, 0.64, 0.65, 0.625, 0.65, 0.635, 0.655, 0.66, 0.65, 0.66, 0.66, 0.675]
# ,[0.6, 0.615, 0.625, 0.66, 0.675, 0.69, 0.7, 0.695, 0.7, 0.725, 0.715, 0.73, 0.74, 0.725, 0.725, 0.74, 0.72, 0.695, 0.715, 0.68, 0.685, 0.675, 0.665, 0.665, 0.68, 0.665, 0.68, 0.615, 0.665, 0.66]
# ,[0.575, 0.595, 0.62, 0.65, 0.675, 0.635, 0.65, 0.655, 0.685, 0.67, 0.69, 0.66, 0.675, 0.66, 0.71, 0.68, 0.7, 0.69, 0.715, 0.72, 0.73, 0.72, 0.705, 0.695, 0.725, 0.715, 0.705, 0.685, 0.705, 0.66]
# ,[0.58, 0.605, 0.635, 0.65, 0.66, 0.68, 0.665, 0.645, 0.635, 0.66, 0.65, 0.685, 0.67, 0.66, 0.67, 0.63, 0.645, 0.66, 0.695, 0.65, 0.605, 0.64, 0.68, 0.65, 0.635, 0.65, 0.685, 0.73, 0.71, 0.695]
# ,[0.615, 0.63, 0.65, 0.66, 0.655, 0.65, 0.66, 0.665, 0.655, 0.66, 0.71, 0.695, 0.695, 0.665, 0.65, 0.635, 0.635, 0.655, 0.63, 0.635, 0.625, 0.675, 0.645, 0.675, 0.66, 0.64, 0.63, 0.64, 0.65, 0.66]
# ,[0.615, 0.625, 0.62, 0.605, 0.58, 0.615, 0.615, 0.63, 0.625, 0.64, 0.665, 0.69, 0.68, 0.71, 0.685, 0.69, 0.695, 0.685, 0.72, 0.705, 0.725, 0.715, 0.695, 0.715, 0.73, 0.725, 0.72, 0.745, 0.725, 0.74]]

# baseline_global = [[0.62, 0.63, 0.645, 0.635, 0.665, 0.71, 0.7, 0.73, 0.715, 0.72, 0.69, 0.73, 0.695, 0.72, 0.7,0.73, 0.71, 0.705, 0.73, 0.72, 0.73, 0.71, 0.71, 0.72, 0.735, 0.7, 0.71, 0.72, 0.72, 0.73]
# ,[0.605, 0.63, 0.655, 0.685, 0.645, 0.635, 0.67, 0.705, 0.69, 0.715, 0.7, 0.715, 0.72, 0.72, 0.725, 0.73, 0.715, 0.725, 0.71, 0.71, 0.72, 0.67, 0.705, 0.715, 0.685, 0.69, 0.665, 0.665, 0.675, 0.68]
# ,[0.59, 0.615, 0.605, 0.68, 0.66, 0.7, 0.715, 0.695, 0.7, 0.695, 0.71, 0.705, 0.715, 0.695, 0.7, 0.68, 0.705, 0.69, 0.695, 0.67, 0.675, 0.675, 0.68, 0.68, 0.69, 0.69, 0.68, 0.685, 0.71, 0.695]
# ,[0.595, 0.605, 0.62, 0.63, 0.66, 0.69, 0.705, 0.7, 0.72, 0.695, 0.675, 0.71, 0.725, 0.715, 0.71, 0.7, 0.715, 0.725, 0.705, 0.71, 0.685, 0.69, 0.69, 0.69, 0.685, 0.68, 0.695, 0.675, 0.66, 0.675]
# ,[0.595, 0.605, 0.61, 0.63, 0.655, 0.69, 0.69, 0.68, 0.695, 0.67, 0.65, 0.655, 0.68, 0.645, 0.7, 0.695, 0.695, 0.7, 0.67, 0.67, 0.665, 0.67, 0.67, 0.655, 0.66, 0.65, 0.675, 0.68, 0.675, 0.675]
# ,[0.58, 0.6, 0.64, 0.655, 0.655, 0.67, 0.67, 0.685, 0.71, 0.695, 0.705, 0.72, 0.74, 0.75, 0.71, 0.715, 0.72, 0.705, 0.69, 0.72, 0.715, 0.705, 0.685, 0.68, 0.69, 0.675, 0.665, 0.69, 0.67, 0.69]
# ,[0.61, 0.595, 0.665, 0.65, 0.665, 0.645, 0.655, 0.67, 0.68, 0.68, 0.675, 0.68, 0.675, 0.66, 0.665, 0.665, 0.68, 0.67, 0.67, 0.64, 0.645, 0.66, 0.655, 0.665, 0.695, 0.69, 0.69, 0.695, 0.68, 0.685]]

# idea4 pretrained D
# [0.609375, 0.65375, 0.6906249999999999, 0.7024999999999999, 0.723125, 0.725625, 0.7374999999999999, 0.739375, 0.75, 0.751875, 0.755, 0.75, 0.7531249999999999, 0.750625, 0.75, 0.753125, 0.7549999999999999, 0.75, 0.746875, 0.74375, 0.74125, 0.73625, 0.73875, 0.738125, 0.7381249999999999, 0.7368750000000001, 0.73625, 0.74, 0.7293749999999999, 0.7387500000000001]
# idea4 D_49
# [0.6049999999999999, 0.6612499999999999, 0.69125, 0.7056250000000001, 0.709375, 0.7175000000000001, 0.73, 0.73375, 0.735, 0.74375, 0.745625, 0.7468749999999998, 0.7431249999999999, 0.744375, 0.7443749999999999, 0.7437500000000001, 0.735, 0.741875, 0.7475, 0.74, 0.7375, 0.7375, 0.735, 0.7324999999999999, 0.7368750000000001, 0.7350000000000001, 0.7318749999999999, 0.7268749999999999, 0.7275, 0.735625]
# PPO
# [0.6221428571428572, 0.6749999999999999, 0.6878571428571428, 0.7092857142857143, 0.7235714285714285, 0.7292857142857143, 0.7335714285714285, 0.7421428571428572, 0.7464285714285713, 0.7507142857142857, 0.7514285714285714, 0.7514285714285714, 0.7457142857142858, 0.7492857142857143, 0.7492857142857143, 0.7542857142857143, 0.7535714285714287, 0.755, 0.7457142857142857, 0.75, 0.7457142857142857, 0.7407142857142858, 0.7421428571428572, 0.7464285714285716, 0.735, 0.7321428571428571, 0.7364285714285715, 0.7478571428571428, 0.7414285714285713, 0.7392857142857142]
# PPO Baseline
success_rate = [[0.63, 0.695, 0.715, 0.725, 0.73, 0.735, 0.735, 0.755, 0.735, 0.75, 0.745, 0.76, 0.755, 0.745, 0.75, 0.75, 0.76, 0.745, 0.755, 0.75, 0.73, 0.75, 0.73, 0.715, 0.735, 0.735, 0.725, 0.71, 0.695, 0.72],
[0.58, 0.655, 0.67, 0.695, 0.7, 0.72, 0.745, 0.735, 0.715, 0.75, 0.765, 0.76, 0.75, 0.755, 0.75, 0.75, 0.75, 0.755, 0.76, 0.74, 0.75, 0.75, 0.74, 0.755, 0.75, 0.76, 0.765, 0.73, 0.74, 0.73],
[0.605, 0.655, 0.705, 0.725, 0.73, 0.725, 0.74, 0.73, 0.74, 0.735, 0.735, 0.74, 0.735, 0.73, 0.735, 0.74, 0.71, 0.735, 0.74, 0.73, 0.715, 0.73, 0.725, 0.715, 0.73, 0.735, 0.72, 0.73, 0.73, 0.73],
[0.615, 0.655, 0.705, 0.715, 0.695, 0.685, 0.7, 0.68, 0.715, 0.725, 0.715, 0.72, 0.72, 0.73, 0.72, 0.725, 0.715, 0.725, 0.745, 0.73, 0.74, 0.72, 0.725, 0.735, 0.74, 0.72, 0.72, 0.715, 0.7, 0.715],
[0.61, 0.69, 0.705, 0.725, 0.72, 0.74, 0.75, 0.765, 0.75, 0.75, 0.775, 0.765, 0.75, 0.765, 0.76, 0.76, 0.735, 0.74, 0.745, 0.75, 0.745, 0.735, 0.745, 0.715, 0.73, 0.74, 0.72, 0.75, 0.73, 0.74],
[0.59, 0.66, 0.69, 0.69, 0.715, 0.73, 0.745, 0.74, 0.76, 0.765, 0.77, 0.75, 0.76, 0.75, 0.755, 0.75, 0.75, 0.76, 0.745, 0.74, 0.735, 0.75, 0.75, 0.755, 0.76, 0.745, 0.75, 0.745, 0.755, 0.755],
[0.615, 0.645, 0.685, 0.695, 0.685, 0.705, 0.715, 0.725, 0.725, 0.73, 0.725, 0.725, 0.73, 0.73, 0.725, 0.735, 0.725, 0.73, 0.74, 0.745, 0.75, 0.725, 0.745, 0.74, 0.73, 0.735, 0.72, 0.71, 0.75, 0.75],
[0.595, 0.635, 0.655, 0.675, 0.7, 0.7, 0.71, 0.74, 0.74, 0.745, 0.735, 0.755, 0.745, 0.75, 0.76, 0.74, 0.735, 0.745, 0.75, 0.735, 0.735, 0.74, 0.72, 0.73, 0.72, 0.71, 0.735, 0.725, 0.72, 0.745]]

idea4 = [0.609375, 0.65375, 0.6906249999999999, 0.7024999999999999, 0.723125, 0.725625, 0.7374999999999999, 0.739375, 0.75, 0.751875, 0.755, 0.75, 0.7531249999999999, 0.750625, 0.75, 0.753125, 0.7549999999999999, 0.75, 0.746875, 0.74375, 0.74125, 0.73625, 0.73875, 0.738125, 0.7381249999999999, 0.7368750000000001, 0.73625, 0.74, 0.7293749999999999, 0.7387500000000001]


baseline_np = np.array(success_rate)
mean = np.mean(baseline_np,axis = 0)
data_df = pd.DataFrame(mean)

# begin writing to xls
writer = pd.ExcelWriter('Save_Excel.xlsx')
data_df.to_excel(writer,'page_1',float_format='%.5f') # float_format 控制精度
writer.save()
print(list(mean))
# begin drawing
axis = [i for i in range(len(mean))]
plt.plot(axis, mean)
plt.plot(axis, idea4)
my_y_ticks = np.arange(0.5, 1, 0.05)
plt.yticks(my_y_ticks)
plt.xlabel('Number of Epoch')
plt.ylabel('Success rate')
plt.show()